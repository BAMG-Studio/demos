# Module 5: Incident Response Simulations - 10 High-Impact Scenarios

## ðŸ“š What is an Incident Response Simulation?

**Technical Definition:**
An incident response simulation (also called a "tabletop exercise" or "red team simulation") is a structured practice scenario where security teams simulate responding to a real security incident without actually having a breach.

**Layman Analogy:**
Incident response simulations are like **fire drills for buildings:**

- **Without drills:** Actual fire happens, people panic, some die, learning is painful
- **With drills:** Practice evacuating regularly, people know exactly what to do, real fire = calm, organized evacuation, everyone survives

**Benefits:**
- âœ… Find gaps in procedures before real incident
- âœ… Train team without stress of real attack
- âœ… Test playbooks and tools
- âœ… Build team confidence
- âœ… Satisfy compliance requirements (PCI-DSS, HIPAA, NIST require regular exercises)

---

## ðŸŽ¯ 10 High-Impact Incident Scenarios

### Scenario 1: Brute Force Attack on Admin Account

**Difficulty:** Easy | **Time:** 30 minutes | **Impact:** HIGH

**The Scenario:**
```
9:00 AM: Alert fires - "10+ failed logins on user admin in 5 minutes"

Timeline of Events:
08:47 AM - Admin working from office
08:55 AM - Admin leaves laptop at desk
09:00 AM - Attacker (nearby) tries password guesses at console
09:00 AM - 1st attempt: password123 (FAIL)
09:00:30 AM - 2nd attempt: Admin@123 (FAIL)
09:01 AM - 3rd attempt: CloudIsSecure (FAIL)
... [10 more attempts] ...
09:05 AM - 11th attempt: AdminPassword (SUCCESS!)
09:05:30 AM - ðŸš¨ ALERT FIRES: 10+ failed logins detected

Questions for Analyst to Answer:
1. Is this real or false positive? (REAL - physical access)
2. How urgent is this? (CRITICAL - admin account)
3. What's the immediate action? (Force password change)
4. Is data compromised? (Check what attacker did after login)
5. How do we prevent? (MFA, account lockout after 5 attempts)
```

**Investigation Steps:**
```
Step 1: Verify the Alert
  SIEM Query: Find failed login events for admin
  Result: Yes, 11 failed logins in 5 minutes from console
  
Step 2: Check for Successful Login
  SIEM Query: Find successful login after failed attempts
  Result: Yes, 1 successful login at 09:05 AM
  
Step 3: Determine Scope
  SIEM Query: Find all actions by admin in last 30 min
  Result:
    - 09:05 AM: Login (successful)
    - 09:06 AM: Accessed S3 bucket (sensitive financial data)
    - 09:07 AM: Downloaded 2 files (SQL backup, customer list)
    - 09:08 AM: Logged out
    
Step 4: Check for Lateral Movement
  SIEM Query: Find if attacker accessed other systems
  Result: No other access detected (contained to admin session)
  
Step 5: Assess Data Breach
  Files downloaded:
    - SQL backup: 500 MB (contains customer PII)
    - Customer list: 5 MB (email addresses, phone numbers)
  Result: CRITICAL - Customer data exposed!
```

**Response Actions:**
```
Immediate (Next 5 minutes):
[ ] Change admin password
[ ] Force logout of admin session
[ ] Enable MFA for admin
[ ] Notify incident commander
[ ] Check if attacker still in building (security cameras)

Short-term (Next 30 minutes):
[ ] Confirm attacker left the premises
[ ] Identify attacker (surveillance footage)
[ ] Notify affected customers (data breach notification)
[ ] Notify legal/regulatory (depends on jurisdiction)
[ ] Begin forensic investigation

Long-term (Next 24 hours):
[ ] Deploy MFA to all admins
[ ] Implement account lockout policy (5 failures = 30 min lock)
[ ] Deploy motion detection on admin workstations
[ ] Review physical security (who has building access?)
[ ] Update incident response playbook
```

**Resume Impact:**
```
"Responded to brute force attack on critical admin account
- Identified attacker (surveillance footage)
- Limited data exposure by responding within 3 minutes
- Determined attacker accessed customer PII (SQLbackup + customer list)
- Executed response playbook (password reset, MFA enablement, access revocation)
- Coordinated multi-team response (security, legal, PR, customers)
- Implemented preventive controls (MFA, account lockout) reducing future risk"
```

---

### Scenario 2: Ransomware Attack on EC2 Instance

**Difficulty:** Medium | **Time:** 1 hour | **Impact:** CRITICAL

**The Scenario:**
```
2:17 AM (Night - only on-call responder)
Alert: "Multiple file deletion attempts on production database instance"

Attacker Timeline:
- 2:00 AM: Attacker gains EC2 access (compromised credentials from LinkedIn)
- 2:05 AM: Launches ransomware payload
- 2:10 AM: Starts encrypting files (/var/www/application, /data/database)
- 2:15 AM: CloudWatch detects unusual process (ransomware crypto operations)
- 2:17 AM: ðŸš¨ ALERT FIRES

Questions for On-Call Engineer:
1. What's happening RIGHT NOW? (Ransomware encrypting files)
2. Can we stop it? (Maybe - isolate instance immediately)
3. How much data lost? (Depends on encryption speed vs. our response time)
4. Do we have backups? (CRITICAL QUESTION!)
5. Should we pay ransom? (NEVER!)
```

**Investigation Steps:**
```
Step 1: Immediate - Isolate Instance
  SIEM: Find instance ID from alert
  Action: Change security group to QUARANTINE (deny all)
  Time: 30 seconds
  Effect: Stops attacker from communicating with C2, accessing S3, etc.
  
Step 2: Assess Encryption Progress
  Check: How much data encrypted? Directories affected?
  CloudWatch: File system activity
  Result: 40% of database encrypted before isolation
    - /var/www/application - 100% encrypted (20 GB)
    - /data/database - 40% encrypted (50 GB of 120 GB)
    - /home/users - Not yet touched
    
Step 3: Check Backups
  Questions:
    - Do we have database backups? (YES - AWS RDS automated backups)
    - How old? (6 hours old - acceptable)
    - Are backups isolated? (Check - can ransomware access them? NO!)
    - Can we restore quickly? (Estimate 30 minutes)
    
Step 4: Analyze Attacker Activity
  SIEM Query: All actions by attacker user in last 3 hours
  Result:
    - 2:00 AM: SSH login (compromised key)
    - 2:01 AM: Executed script (ransomware)
    - 2:05 AM: Started crypto process
    - 2:15 AM: Attempted S3 access (blocked by SCP!)
    - 2:16 AM: Attempted RDS access (blocked by security group!)
    => Good news: Attacker contained by security controls!
    
Step 5: Check for Lateral Movement
  Did attacker access other systems?
  Result: Instance is isolated on private subnet, no access to other systems
  => More good news: Blast radius limited to one instance!
```

**Response Actions:**
```
Immediate (Next 5 minutes):
[X] Isolate instance (done - taken ~30 seconds)
[X] Stop encryption by isolation (done)
[ ] Create forensic snapshot of infected instance
[ ] Notify incident commander
[ ] Get approval for recovery actions
[ ] Page database administrator (DBA)

Short-term (Next 30 minutes):
[ ] Determine if paying ransom is possible (Don't recommend!)
[ ] Check: Can we restore from backup without paying?
[ ] Initiate RDS restore from 6-hour-old backup:
    aws rds restore-db-instance-to-point-in-time \
      --source-db-instance-identifier production-db \
      --db-instance-identifier production-db-recovered \
      --restore-time 2025-10-28T20:17:00Z
[ ] Monitor restore progress (estimated 30 minutes)
[ ] Update DNS to point to recovered database
[ ] Test application connectivity
[ ] Verify data integrity (spot checks)

Long-term (Next 24 hours):
[ ] Forensic analysis of ransomware (what type? reversible?)
[ ] Identify how credentials were compromised (LinkedIn hack!)
[ ] Rotate all AWS credentials
[ ] Scan all instances for same ransomware
[ ] Deploy EDR (Endpoint Detection & Response) tool
[ ] Implement file integrity monitoring
[ ] Update incident response playbook
[ ] Post-incident review with team

Communication:
[ ] Notify customers? (Depends on data exposure - in this case NO)
[ ] Notify regulatory? (Depends on compliance requirements)
[ ] Notify law enforcement? (FBI has ransomware taskforce)
[ ] Notify insurance? (Cyber insurance may cover costs)
```

**Damage Assessment:**
```
Data Lost: 1.5 days of activity (6-hour-old backup restores to 20:17 yesterday)
  Impact: ~100 transactions lost, customers will notice
  
Downtime: ~45 minutes (15 min response + 30 min restore)
  Cost: If service makes $1M/hour = $750K downtime cost
  
Good news: No data exfiltration (attacker couldn't access S3 or backups)
  Impact: No customer data breach, no regulatory fine, no PR disaster
  
Without good response: Would have paid $100K+ ransom!
  With good response: Limited to $750K downtime cost + some customer impact
  
Savings: $100K ransom avoided, ransomware research shows only 4% who pay get data!
```

**Resume Impact:**
```
"Responded to critical ransomware attack on production database
- Detected attack within 2 minutes of encryption start
- Isolated affected instance within 30 seconds (preventing lateral spread)
- Assessed backup availability and recovery options
- Executed database restore from clean backup (30 minutes)
- Identified attack vector (compromised LinkedIn credentials)
- Prevented $100K+ ransom payment through rapid recovery
- Implemented post-incident controls (EDR, file integrity monitoring)
- Led team through organized response despite night-time incident"
```

---

### Scenario 3: Insider Threat - Employee Stealing Data

**Difficulty:** Hard | **Time:** 2 hours | **Impact:** CRITICAL

**The Scenario:**
```
10:30 AM: Alert - "Unusual S3 data access by analyst user"

The Story:
- Employee: Alice, Data Analyst (1 year tenure, good reviews)
- Access: Should only access sanitized customer dataset (10 GB)
- Alert: Accessing RAW customer database (500 GB PII - passwords, SSNs, credit cards)
- Activity: Downloaded 50 GB in 30 minutes to external IP
- Motivation: Job offer from competitor, needs to bring "portfolio" to new job

Timeline:
09:00 AM - Alice receives job offer (2x salary)
09:30 AM - Alice emails recruiter: "How much data can I share?"
10:00 AM - Alice starts downloading sensitive data
10:25 AM - Downloaded 50 GB
10:30 AM - ðŸš¨ SIEM Alert fires (unusual access pattern)
10:31 AM - Security team investigates
```

**Investigation Steps:**
```
Step 1: Verify Alert
  SIEM Query: S3 access by alice user in last hour
  Result: 
    - 10:00 AM - 10:25 AM: 50 consecutive GetObject calls to raw-customer-data bucket
    - Unusual pattern: Normally accesses only sanitized-data bucket
    - Traffic: 50 GB downloaded (10x normal daily usage)
    
Step 2: Assess Data Exposure
  Questions:
    - What data accessed? (Raw customer PII - 500K customers)
    - How much? (50 GB - full customer database)
    - Where sent? (IP 203.0.113.50 - looks like home/coffee shop)
    - Still ongoing? (No - stopped at 10:25 AM, ~30 min duration)
    
Step 3: Determine Intent
  Check employee email (with legal approval):
    - Email to external recruiter: "Will bring proprietary data"
    - Resume update: "Salary 2x higher, effective next month"
    - LinkedIn: Job search active
    Result: Intentional, deliberate data theft (not accident!)
    
Step 4: Assess Legal Situation
  Data exposed:
    - 500,000 customers' PII (Passwords, SSNs, Credit cards)
    - Regulatory impact: CCPA, GDPR, HIPAA
    - Notification requirement: All 500K customers
    - Fines: $10,000+ per customer ($5M+) under GDPR
    - Criminal liability: Data theft = felony
    
Step 5: Forensic Analysis
  Questions:
    - How did she get credentials? (Legitimate job access)
    - Did she remove data from laptop? (Yes - cloud -> home drive)
    - Did she upload to cloud service? (Check cloud provider logs)
    - Any accomplices? (Investigate other data analysts)
    - Previous incidents? (Check historical access logs)
```

**Response Actions:**
```
IMMEDIATE - First 5 minutes:
[ ] Confirm alert is real (quick verification)
[ ] Disable Alice's AWS credentials/IAM user
   - Force logout of all sessions
   - Rotate access keys
   - Remove from all groups
[ ] Isolate her laptop (physical or remote lock)
[ ] Notify HR and legal (cannot discuss with employee yet!)
[ ] Preserve all evidence (emails, logs, access patterns)

SHORT-TERM - First 30 minutes:
[ ] Verify full scope of data exposure:
   - Which S3 buckets accessed?
   - What time range?
   - How much data total?
   - Any other users involved?
[ ] Check cloud storage services (Google Drive, OneDrive, Dropbox):
   - Was data uploaded to personal cloud?
   - Can we request removal?
[ ] Check external IP logs:
   - Was data transferred to competitor?
   - Other suspicious activity?
[ ] Involve law enforcement (FBI / local police):
   - Data theft is a FEDERAL CRIME
   - Computer Fraud and Abuse Act (CFAA)
   - Wire fraud (if sent across state lines)

MEDIUM-TERM - Next few hours:
[ ] Notify customers (legal requirement):
   - Data breach notification letters
   - Credit monitoring offer (60 months free)
   - Regulatory notifications (state AGs, GDPR authority, etc.)
[ ] Coordinate with HR:
   - Terminate employment
   - Legal hold on her devices
   - Prevent her from talking to colleagues
[ ] Investigation:
   - Forensic analysis of her laptop
   - Review all access by other data analysts
   - Check if others accessed same data
   - Interview colleagues (did she seem interested in other companies?)
[ ] Damage control:
   - Public statement (PR/Communications)
   - Customer support for questions
   - Credit monitoring setup for victims

LONG-TERM:
[ ] Change all S3 credentials
[ ] Rotate all cloud credentials that Alice had access to
[ ] Implement DLP (Data Loss Prevention) tools:
   - Monitor downloads of sensitive data
   - Block unauthorized cloud uploads
   - Track removable media usage
[ ] Review access controls (principle of least privilege):
   - Data analysts should NOT access full raw customer data
   - Implement role-based access control (RBAC)
   - Require data anonymization for analysts
[ ] Audit logs:
   - Who else accessed raw customer data?
   - Are there other insider threats?
   - What's normal access pattern?
[ ] Update incident response playbook for insider threats
[ ] Privacy impact assessment + regulatory filings
```

**Business Impact:**
```
Direct Costs:
  - Data breach notification: $500K (print + postage + credit monitoring)
  - GDPR fines: $5M+ (20M EUR max)
  - Legal costs: $500K+ (attorney, law enforcement)
  - Credit monitoring: $60 per customer * 500K = $30M (often by company!)
  TOTAL: $35M+ in direct costs

Indirect Costs:
  - Customer trust loss (some will leave)
  - Reputational damage (news coverage)
  - Stock price impact (if public company)
  - Employee confidence loss (insider threat concerns)
  
Prevention Cost: $100K-$500K in DLP tools/monitoring
  => ROI: 100:1 (spending $500K to prevent $50M loss)
```

**Resume Impact:**
```
"Detected and investigated insider threat data theft incident
- Identified unauthorized access pattern through SIEM alerting
- Correlated user behavior with credential misuse
- Coordinated rapid response (disabled access within 5 minutes)
- Preserved forensic evidence for law enforcement
- Assisted with customer notification and regulatory compliance
- Recommended and implemented DLP controls
- Led security culture change (insider threat awareness)
- Delivered training to prevent future incidents"
```

---

### Scenario 4: Supply Chain Attack - Compromised Dependency

**Difficulty:** Very Hard | **Time:** 2-3 hours | **Impact:** CRITICAL

**The Scenario:**
```
3:42 AM: Alert - "Unusual outbound traffic from production Lambda function"

The Story:
- Your application uses Node.js npm package: "popular-logging-tool" (10K downloads/day)
- Package is maintained by external developer
- Developer's GitHub account gets compromised
- Attacker publishes malicious version (v2.3.1 with backdoor)
- Your CI/CD auto-updates dependencies (latest version = vulnerability!)
- Malicious code: Steals AWS credentials from function environment
- Credentials sent to attacker's server in China

Timeline:
Tuesday 10 AM - Attacker compromises developer's GitHub account
Tuesday 11 AM - Publishes malicious version v2.3.1
Tuesday 11:15 AM - Your CI/CD runs, auto-updates package
Tuesday 11:30 AM - Deploy pipeline updates to Lambda
Tuesday 11:31 AM - Malicious code executes, steals credentials
Tuesday 11:32 AM - Credentials exfiltrated to attacker's server
... [3 hours of undetected compromise] ...
Wednesday 2:30 AM - Developer notices fork activity, investigates
Wednesday 3:42 AM - Posts security advisory on npm
Wednesday 3:43 AM - Your monitoring system alerts on unusual traffic
```

**Investigation Steps:**
```
Step 1: Identify Anomaly
  Alert Details:
    - Lambda function: user-authentication-service
    - Destination IP: 203.0.113.200 (China)
    - Traffic: HTTPS POST with credentials-like data
    - Volume: 1 MB outbound (very unusual for this function)
  
Step 2: Check Lambda Recent Deployments
  Git history:
    - Deploy v4.2.0 at 11:30 AM Tuesday
    - Major dependency update: popular-logging-tool v2.2.5 â†’ v2.3.1
    - Change: Auto-update dependencies enabled
    - Review: No manual code review of dependency changes
  
Step 3: Investigate Dependency
  Check npm package:
    - popular-logging-tool v2.3.1 published Tuesday 11 AM
    - Behavior: Makes outbound HTTPS call during initialization
    - Destination: 203.0.113.200 (attacker server)
    - Payload: Exfiltrates AWS credentials from Lambda environment
  
Step 4: Assess Exposure
  Credentials stolen:
    - AWS_ACCESS_KEY_ID: AKIAJO...
    - AWS_SECRET_ACCESS_KEY: ...
    - Role: production-lambda-execution-role
    
  Permissions on role:
    - Read: S3 buckets (customer data)
    - Write: DynamoDB (customer accounts)
    - Create: EC2, Lambda (launch new resources!)
    - Delete: Anything in production!
  
  Attack potential: UNLIMITED (admin-level permissions!)
  
Step 5: Determine Attacker Activity
  CloudTrail Query: All actions by this role in last 3 hours
  Result:
    - Probably downloading customer data from S3 (thousands of GetObject calls)
    - Possibly created backdoor user
    - Possibly launched mining instances (crypto mining)
  
  Damage assessment: SEVERE (customer data + infrastructure compromise)
```

**Response Actions:**
```
IMMEDIATE - First 10 minutes:
[ ] Disable compromised credentials:
    aws iam delete-access-key --access-key-id AKIAJO...
    
[ ] Disable Lambda function:
    aws lambda delete-function --function-name user-authentication-service
    => This will cause service outage, but better than breach
    
[ ] Kill any suspicious processes:
    Look for: EC2 instances (mining), new users (backdoors)
    
[ ] Notify incident commander and CTO:
    "CRITICAL: Supply chain attack detected, Lambda credentials stolen"

SHORT-TERM - First 30 minutes:
[ ] Rollback to previous code version:
    - Revert package.json to safe version
    - Redeploy without malicious dependency
    - Verify Lambda restored and functional
    
[ ] Identify all affected services:
    - Which other services use compromised package?
    - Search all repositories for "popular-logging-tool"
    - Prepare rollback for each service
    
[ ] Assess damage:
    - How long was backdoor active? (3+ hours = significant)
    - CloudTrail: All actions by stolen credentials
    - S3 access logs: Data downloaded?
    - Did attacker create backdoors? (new users, new keys)
    
[ ] Kill attacker's access:
    - Change password on production-lambda-execution-role
    - Create new access key for future use
    - Revoke old credentials completely

[ ] Secure development pipeline:
    - Disable auto-update of dependencies!
    - Require manual review for dependency changes
    - Implement "dependency locking" (pin versions)
    - Add security scanning to CI/CD pipeline

MEDIUM-TERM:
[ ] Public communication:
    - Notify customers (data breach?)
    - Post on security advisory channels
    - Coordinate with npm and package maintainer
    - Monitor for other compromised packages
    
[ ] Investigation:
    - Package maintainer: Why was GitHub compromised?
    - Other services affected: How many?
    - Supply chain risk: Can we trust dependencies?
    
[ ] Implement controls:
    - Dependency scanning (check for known vulnerabilities)
    - SBOM (Software Bill of Materials) - track all dependencies
    - Approved vendor list - only use trusted packages
    - Vendor security assessments
    
[ ] Long-term changes:
    - Remove dependency if possible (can we write logging ourselves?)
    - Use alternative maintainer (with better security)
    - Implement internal package mirror (scan before use)
    - Regular security training (supply chain risks)
```

**Business Impact:**
```
Customer Data Exposed:
  - 100,000+ customers' data accessed by attacker
  - Data: Usernames, passwords (hashed but...?), payment methods
  - Regulatory: CCPA, GDPR, PCI-DSS all violated
  - Fines: $5M+
  
Service Downtime:
  - Rolled back Lambda (minutes of outage)
  - Customer impact: Low (rollback was quick)
  
Crypto Mining (if attacker used credentials to launch instances):
  - Cost: $10K+ in AWS charges before detection
  
Backdoor (if attacker created users):
  - Risk: Months of ongoing compromise
  - Cost: Undefined (depends on attacker's plans)
  
Prevention Cost: $50K in security tooling
  => ROI: Very high (prevents multi-million dollar breach)
```

**Resume Impact:**
```
"Detected and responded to supply chain attack via compromised npm package
- Identified suspicious outbound traffic from Lambda function
- Correlated with recent dependency update
- Rapidly deactivated compromised credentials (5 minutes to detection/response)
- Assessed scope of exposure across infrastructure
- Coordinated with multiple teams (dev, security, platform)
- Implemented preventive controls (dependency scanning, pinning, approval process)
- Developed security awareness training on supply chain risks
- Documented incident and led post-incident review
- Estimated: Prevented $5M+ data breach through rapid response"
```

---

### Scenario 5: API-Based Attack (Lateral Movement & Data Theft)

**Difficulty:** Medium | **Time:** 1 hour | **Impact:** CRITICAL

**The Scenario:**
```
2:15 PM: Unusual API traffic detected from sales application

Timeline of Events:
01:30 PM - Attacker compromised sales app (via vulnerable dependency)
01:45 PM - Attacker discovers API key hardcoded in source code
02:00 PM - Attacker uses API key to call finance API
02:15 PM - ðŸš¨ ALERT: "Finance API called from unusual IP (attacker location)"
02:30 PM - Analyst investigating sees 10,000 API calls in 2 minutes
02:45 PM - Customer data being downloaded (accounts, balances, transactions)

Investigation Questions:
1. How did they get the API key? (Code review? GitHub? Secrets manager?)
2. What data is accessible? (Run query: SELECT * FROM API_LOGS WHERE key=?)
3. How many records were accessed? (Check logs for data volume)
4. Which customers affected? (Identify account IDs from API calls)
5. Can we stop it? (Immediately revoke API key)
6. Did they exfiltrate? (Check S3 download logs, network egress)

Playbook Steps:
â”œâ”€ DETECT (2 min)
â”‚  â””â”€ API gateway logs show unusual volume
â”‚  â””â”€ CloudWatch dashboard shows spike
â”‚  â””â”€ GuardDuty may alert on unusual activity
â”‚
â”œâ”€ CONTAIN (5 min)
â”‚  â”œâ”€ Revoke compromised API key
â”‚  â”‚  $ aws apigateway update-api-key --api-key [KEY] --enabled false
â”‚  â”œâ”€ Disable sales app (stop further compromise)
â”‚  â”‚  â””â”€ EC2 Security Group: Remove all inbound rules
â”‚  â””â”€ Block attacker IP at WAF level
â”‚     â””â”€ AWS WAF: Add IP to blocklist
â”‚
â”œâ”€ INVESTIGATE (20 min)
â”‚  â”œâ”€ CloudTrail: Who called which APIs?
â”‚  â”œâ”€ S3 access logs: Was data downloaded?
â”‚  â”œâ”€ VPC Flow Logs: Which IPs accessed which APIs?
â”‚  â”œâ”€ Query: How many records per API?
â”‚  â”‚  SELECT api_endpoint, COUNT(*) FROM api_logs WHERE timestamp > NOW() - INTERVAL 30 MINUTE
â”‚  â””â”€ Identify: Customer accounts affected
â”‚     SELECT DISTINCT customer_id FROM api_logs WHERE key = 'COMPROMISED_KEY'
â”‚
â”œâ”€ REMEDIATE (15 min)
â”‚  â”œâ”€ Create new API key
â”‚  â”œâ”€ Rotate credentials in secrets manager
â”‚  â”œâ”€ Update source code (remove hardcoded keys)
â”‚  â”œâ”€ Deploy updated app
â”‚  â””â”€ Re-enable with new key
â”‚
â””â”€ COMMUNICATE (10 min)
   â”œâ”€ Calculate: 50,000 customer records accessed
   â”œâ”€ Notify: CISO, Legal, Privacy officer
   â”œâ”€ Prepare: Customer notification email (required by GDPR/CCPA)
   â”‚  â””â”€ "On [date] unauthorized access to customer data occurred"
   â”‚  â””â”€ "Customers affected: 50,000"
   â”‚  â””â”€ "Data accessed: names, account numbers (not passwords, SSNs encrypted)"
   â”‚  â””â”€ "Steps taken: Key revoked, app patched, monitoring enhanced"
   â”‚  â””â”€ "Free credit monitoring: Yes"
   â””â”€ Cost: GDPR fine = $20M Ã— 4% = $800K minimum

Total Time: 1 hour (Alert â†’ Blocked)

Resume Bullets:
â€¢ "Detected and responded to API-based data breach in <1 hour, protecting 50,000 customers"
â€¢ "Identified hardcoded secrets in source code, implemented secrets management solution"
â€¢ "Implemented API rate limiting preventing similar attacks, 60% reduction in suspicious API calls"
â€¢ "Established automated API key rotation, eliminated manual credential management"
```

---

### Scenario 6: DDoS Attack (Availability & Impact)

**Difficulty:** Medium | **Time:** 1 hour | **Impact:** HIGH (Revenue loss)

**The Scenario:**
```
3:00 PM: Website unavailable - "Connection timed out"

Timeline of Events:
02:50 PM - Attacker launches DDoS (Botnet with 100,000 machines)
02:55 PM - Website slow (1 sec response time â†’ 10 sec)
03:00 PM - ðŸš¨ ALERT: "ALB receives 100K requests/sec (normal = 5K/sec)"
03:00 PM - Customers complain: "Website down"
03:05 PM - Twitter: #CompanyDown trending
03:10 PM - Revenue tracker: $50K/min loss for e-commerce
03:15 PM - Media: "Company's website hacked?"
03:30 PM - Attacker email: "Pay $100K or keep attacking"

Investigation & Response:
â”œâ”€ DETECT (1 min)
â”‚  â””â”€ CloudWatch: 1000% traffic spike
â”‚  â””â”€ ALB: 99% of requests from 10 IPs (botnet)
â”‚  â””â”€ Monitoring alert: "Availability < 50%"
â”‚
â”œâ”€ CONTAIN (5 min)
â”‚  â”œâ”€ Enable AWS DDoS protection (Shield Standard = free)
â”‚     â””â”€ Already included, automatic mitigation starts
â”‚  â”œâ”€ Enable AWS Shield Advanced ($3K/month)
â”‚     â””â”€ Provides DDoS specialists on-call
â”‚     â””â”€ Insurance against DDoS costs ($25K deductible)
â”‚  â”œâ”€ Enable AWS WAF
â”‚     â”‚â”€ Rate limiting: Max 2000 requests per IP per minute
â”‚     â”‚â”€ Geo-blocking: Block requests from known botnet countries
â”‚     â”‚â””â”€ $5/month per rule
â”‚  â””â”€ Scale up capacity (if not botnet)
â”‚     â””â”€ ALB Auto Scaling: Increase to max instances
â”‚
â”œâ”€ INVESTIGATE (10 min)
â”‚  â”œâ”€ Is this real DDoS or broken code?
â”‚  â”‚  â””â”€ Check: Are requests legitimate? (Check User-Agent, origin)
â”‚  â”‚  â””â”€ Normal user agents vs. all robot user agents = Botnet
â”‚  â”œâ”€ Where's traffic from?
â”‚  â”‚  â””â”€ VPC Flow Logs: Top source IPs
â”‚  â”‚  â””â”€ Almost all from non-US? Likely botnet
â”‚  â”œâ”€ What endpoints targeted?
â”‚  â”‚  â””â”€ ALB logs: GET / â†’ 10K req/sec (all homepage)
â”‚  â”‚  â””â”€ GET /api/login â†’ 20K req/sec
â”‚  â”‚  â””â”€ POST /purchase â†’ down from 500 to 100 per sec
â”‚
â”œâ”€ REMEDIATE (20 min)
â”‚  â”œâ”€ Activate DDoS playbook
â”‚  â”œâ”€ Contact AWS DDoS response team
â”‚  â”œâ”€ Implement aggressive WAF rules
â”‚  â”‚  â””â”€ Only allow requests with valid auth token
â”‚  â”‚  â””â”€ Rate limit: 100 requests per second per IP
â”‚  â”‚  â””â”€ Block: All requests from known botnet ASNs
â”‚  â”œâ”€ Scale infrastructure
â”‚  â”‚  â””â”€ Increase ALB capacity 10x
â”‚  â”‚  â””â”€ Enable EC2 Auto Scaling to 100 instances
â”‚  â””â”€ Switch to CDN (CloudFront)
â”‚     â””â”€ Cache homepage, API responses
â”‚     â””â”€ Filter botnet at edge (before hitting ALB)
â”‚
â””â”€ COMMUNICATE (10 min)
   â”œâ”€ Status page: "We're aware of slowness, investigating"
   â”œâ”€ Twitter: "We're experiencing traffic spike, working on it"
   â”œâ”€ Customers: Email "Service disruption 3:00-3:45 PM, sorry!"
   â”œâ”€ CEO: "Our website was down for 45 min, lost $2.25M in sales"
   â”œâ”€ Media: "DDoS attack", but NOT payment extortion demand (don't negotiate)
   â””â”€ Costs:
      â”œâ”€ Revenue lost: $2.25M
      â”œâ”€ AWS overage: Extra servers = $50K
      â”œâ”€ DDoS mitigation services: Included if Shield Advanced (had $3K/month)
      â”œâ”€ SLA credits: -$500K (90.0% uptime â†’ customer refunds)
      â””â”€ TOTAL COST: $2.75M

Resume Bullets:
â€¢ "Responded to DDoS attack impacting 100K+ users, restored service in <1 hour"
â€¢ "Implemented AWS WAF rate limiting, prevented 98% of DDoS traffic post-incident"
â€¢ "Deployed CloudFront caching architecture, improved site availability from 95% to 99.9%"
â€¢ "Established DDoS response playbook, reduced MTTR from 2 hours to 15 minutes"
```

---

### Scenario 7: Cloud Misconfiguration (Unintended Data Exposure)

**Difficulty:** Easy | **Time:** 30 minutes | **Impact:** CRITICAL (Data breach)

**The Scenario:**
```
10:30 AM: Security researcher reports: "I can download your entire database from S3"

Timeline of Events:
09:00 AM - Developer creates S3 bucket for backups
09:05 AM - Thinks: "I'll configure security later"
09:10 AM - Uploads database backup (1 million customer records)
09:15 AM - Thinks: "I'll make it public temporarily for testing"
09:16 AM - ðŸš  MISTAKE: Changed bucket policy to "AllowPublicRead"
09:17 AM - Bucket is now PUBLICLY READABLE on the internet
09:18-10:29 AM - Unknown attackers discovering and downloading database
10:30 AM - ðŸš¨ White hat security researcher reports it
10:31 AM - We find it public, 50K customers affected
11:00 AM - Breach disclosed to customers

Investigation:
â”œâ”€ DETECT (2 min)
â”‚  â””â”€ AWS Config rule: "S3 bucket public access" fires
â”‚  â””â”€ Alert: "Bucket [name] is publicly readable"
â”‚
â”œâ”€ INVESTIGATE (10 min)
â”‚  â”œâ”€ Confirm: Yes, bucket is public
â”‚  â”œâ”€ What's in it? 1.2 GB database backup
â”‚  â”‚  â””â”€ SELECT COUNT(*) FROM RECORDS
â”‚  â”‚  â””â”€ Result: 1,000,000 customer records
â”‚  â”œâ”€ What data? names, addresses, card tokens, SSNs (masked but exposed)
â”‚  â”œâ”€ Who accessed? 
â”‚  â”‚  â””â”€ S3 access logs show 500+ unique IPs
â”‚  â”‚  â””â”€ 50 GB downloaded (estimated 250K records stolen)
â”‚  â””â”€ When first exposed?
â”‚     â””â”€ CloudTrail: Policy changed 10:16 AM
â”‚     â””â”€ First download: 10:18 AM
â”‚
â”œâ”€ CONTAIN (5 min)
â”‚  â”œâ”€ Remove public access immediately
â”‚  â”‚  $ aws s3api put-bucket-acl --bucket name --acl private
â”‚  â”œâ”€ Delete exposed backup
â”‚     $ aws s3 rm s3://bucket/backup.sql
â”‚  â””â”€ Revoke any temp credentials
â”‚
â”œâ”€ REMEDIATE (20 min)
â”‚  â”œâ”€ Implement AWS Config rules
â”‚  â”‚  â””â”€ "s3-bucket-public-read-prohibited" (REQUIRED)
â”‚  â”‚  â””â”€ "s3-bucket-public-write-prohibited" (REQUIRED)
â”‚  â”œâ”€ Implement SCP (Service Control Policy)
â”‚  â”‚  â””â”€ Deny: s3:PutBucketAcl (prevent public access)
â”‚  â”‚  â””â”€ Deny: s3:PutBucketPolicy (prevent public policy)
â”‚  â”œâ”€ Encrypt all S3 buckets
â”‚  â”‚  $ aws s3api put-bucket-encryption --bucket name --server-side-encryption-configuration '{...}'
â”‚  â”œâ”€ Enable versioning (can recover deleted files)
â”‚     $ aws s3api put-bucket-versioning --bucket name --versioning-configuration Status=Enabled
â”‚  â””â”€ Update backup procedures
â”‚     â”œâ”€ Use AWS Backup (managed backup service)
â”‚     â”œâ”€ Automated encryption
â”‚     â”œâ”€ Locked retention (can't delete for 30 days)
â”‚     â””â”€ Private by default
â”‚
â””â”€ COMMUNICATE (15 min)
   â”œâ”€ Notify: CISO, Legal, Privacy officer
   â”œâ”€ Regulatory notification:
   â”‚  â””â”€ GDPR: Must notify within 72 hours
   â”‚  â””â”€ CCPA: Must notify customers
   â”‚  â””â”€ HIPAA: If health data, within 60 days
   â”œâ”€ Customer notification:
   â”‚  â””â”€ Letter: "Unintended public exposure of data"
   â”‚  â””â”€ "Approximately 250,000 customers affected"
   â”‚  â””â”€ "Data exposed: name, address, SSN, card token"
   â”‚  â””â”€ "We have offered free credit monitoring"
   â”‚  â””â”€ "No evidence of fraudulent activity to date"
   â”œâ”€ Regulatory fines:
   â”‚  â””â”€ GDPR: 2-4% of annual revenue (up to â‚¬20M)
   â”‚  â””â”€ CCPA: $100-750 per customer (worst case = $75M+)
   â”‚  â””â”€ HIPAA: $100-50K per record (worst case = $50B+)
   â””â”€ Crisis management:
      â”œâ”€ Media statement: "We identified a misconfiguration"
      â”œâ”€ Press release: "We're taking corrective action"
      â””â”€ Cost: $50M+ (fines) + $10M (credit monitoring) + reputation damage

Resume Bullets:
â€¢ "Identified and remediated S3 public exposure affecting 250K customers in <30 minutes"
â€¢ "Implemented AWS Config rules to prevent public S3 bucket exposure, 100% compliance"
â€¢ "Created automated backup security framework with encryption, versioning, and audit logging"
â€¢ "Trained development team on secure cloud configuration, eliminating similar misconfigurations"
```

---

### Scenario 8: Compliance Violation (Audit Failure)

**Difficulty:** Medium | **Time:** 2 hours | **Impact:** HIGH (Regulatory)

**The Scenario:**
```
9:00 AM: External auditor reports: "You're not compliant with HIPAA"

Timeline of Events:
08:30 AM - Annual HIPAA audit starts
09:00 AM - ðŸš¨ FINDING: "CloudTrail not enabled on all accounts"
09:15 AM - FINDING: "RDS database not encrypted"
09:30 AM - FINDING: "S3 access logs not enabled"
09:45 AM - FINDING: "No MFA for console access" (10 users)
10:00 AM - FINDING: "IAM policy allows 's3:*' (overly permissive)"
10:30 AM - Total findings: 15 critical, 25 major, 40 minor
11:00 AM - Auditor: "You're not compliant, HIPAA violation"
11:15 AM - Possible penalties: $100 per record Ã— 1M patients = $100M fine

Investigation & Response:
â”œâ”€ PHASE 1: TRIAGE (30 min)
â”‚  â”œâ”€ Categorize findings
â”‚  â”‚  â”œâ”€ Critical (15): Must fix before date X
â”‚  â”‚  â”œâ”€ Major (25): Fix in next 30 days
â”‚  â”‚  â””â”€ Minor (40): Fix in next 90 days
â”‚  â”œâ”€ Assess risk: Which findings most likely to cause breach?
â”‚  â”‚  â””â”€ Top 3 risks:
â”‚  â”‚     1. Unencrypted RDS (patient data at risk)
â”‚  â”‚     2. No CloudTrail (can't prove compliance)
â”‚  â”‚     3. Weak IAM (anyone can access anything)
â”‚  â””â”€ Create remediation plan
â”‚
â”œâ”€ PHASE 2: IMMEDIATE REMEDIATION (1 hour)
â”‚  â”œâ”€ Critical Finding #1: Enable RDS encryption
â”‚  â”‚  â”œâ”€ Problem: 50 TB database, must encrypt
â”‚  â”‚  â”œâ”€ Solution: AWS DMS (Database Migration Service)
â”‚  â”‚  â”‚  1. Create new encrypted RDS
â”‚  â”‚  â”‚  2. Migrate data (takes 2-4 hours for large DB)
â”‚  â”‚  â”‚  3. Update connection strings
â”‚  â”‚  â”‚  4. Cutover to new database
â”‚  â”‚  â””â”€ Risk: Potential downtime during cutover
â”‚  â”‚
â”‚  â”œâ”€ Critical Finding #2: Enable CloudTrail on all accounts
â”‚  â”‚  â”œâ”€ Problem: 100 accounts, no audit logging
â”‚  â”‚  â”œâ”€ Solution: Organization trail (1 trail covers all)
â”‚  â”‚  â”‚  $ aws cloudtrail create-trail --name org-trail --s3-bucket-name audit-logs --is-organization-trail
â”‚  â”‚  â”‚  $ aws cloudtrail start-logging --trail-name org-trail
â”‚  â”‚  â””â”€ Time: 15 minutes for all accounts
â”‚  â”‚
â”‚  â”œâ”€ Critical Finding #3: Enforce MFA on console
â”‚  â”‚  â”œâ”€ Problem: 10 users without MFA
â”‚  â”‚  â”œâ”€ Solution: Require MFA (SCP policy)
â”‚  â”‚  â”‚  "Deny": ["iam:CreateLoginProfile","iam:UpdateLoginProfile"]
â”‚  â”‚  â”‚  "Condition": {"Bool": {"aws:MultiFactorAuthPresent": false}}
â”‚  â”‚  â””â”€ Time: 1 hour (educate users on MFA setup)
â”‚  â”‚
â”‚  â””â”€ Total time to fix critical findings: 3-4 hours
â”‚
â”œâ”€ PHASE 3: SECONDARY REMEDIATION (Next 30 days)
â”‚  â”œâ”€ Major Finding: S3 access logging
â”‚  â”‚  â””â”€ Enable on all buckets with patient data
â”‚  â”œâ”€ Major Finding: IAM policy review
â”‚  â”‚  â””â”€ Change from 's3:*' to specific actions
â”‚  â”œâ”€ Major Finding: VPC Flow Logs
â”‚  â”‚  â””â”€ Enable on all VPCs
â”‚  â””â”€ Major Finding: Secrets rotation
â”‚     â””â”€ Implement automated rotation
â”‚
â””â”€ PHASE 4: DOCUMENTATION (2+ hours)
   â”œâ”€ Create evidence package
   â”‚  â”œâ”€ CloudTrail exports (prove CloudTrail enabled)
   â”‚  â”œâ”€ AWS Config reports (show compliance state)
   â”‚  â”œâ”€ Security Hub findings (automated compliance)
   â”‚  â”œâ”€ Access logs (prove data not accessed)
   â”‚  â””â”€ Remediation timeline (fixed by this date)
   â”œâ”€ Update HIPAA documentation
   â”‚  â”œâ”€ Business Associate Agreement (BAA)
   â”‚  â”œâ”€ Security Risk Assessment
   â”‚  â”œâ”€ Incident Response Plan
   â”‚  â”œâ”€ Business Continuity Plan
   â”‚  â””â”€ Workforce Security procedures
   â””â”€ Prepare for re-audit
      â””â”€ Schedule 30-90 days out

Total remediation cost:
â”œâ”€ AWS costs: $50K (DMS, extra resources, migration)
â”œâ”€ Personnel: 400 hours Ã— $200/hr = $80K
â”œâ”€ Tools: AWS Config, Security Hub, other = $30K
â”œâ”€ Potential fine (if negotiated): $100K-$1M
â””â”€ TOTAL: $260K-$1.16M (vs. $100M fine if ignored)

Resume Bullets:
â€¢ "Led HIPAA audit remediation, achieving 100% compliance within 30 days"
â€¢ "Implemented organization-wide CloudTrail logging across 100 AWS accounts"
â€¢ "Enforced MFA on all console users, eliminated non-compliant access patterns"
â€¢ "Created continuous compliance monitoring with AWS Config, reducing audit findings 85%"
```

---

### Scenario 9: Multi-Stage Attack (Kill Chain)

**Difficulty:** Hard | **Time:** 3+ hours | **Impact:** CRITICAL

**The Scenario:**
```
Attack uses multiple techniques in sequence (kill chain):
1. Initial Access â†’ 2. Persistence â†’ 3. Lateral Movement â†’ 4. Credential Access â†’ 5. Exfiltration

Timeline of Events:
Day 1, 2:00 PM: Attacker sends phishing email
Day 1, 2:15 PM: Developer clicks link (Initial Access)
Day 1, 2:20 PM: Malware installs backdoor (Persistence)
Day 1, 3:00 PM: Attacker logs in via backdoor (Persistence)
Day 1, 3:15 PM: Attacker moves to finance server (Lateral Movement)
Day 1, 3:30 PM: Attacker steals finance credentials (Credential Access)
Day 1, 4:00 PM: Attacker logs in as finance user
Day 1, 5:00 PM: Attacker accesses database (T1530 - Collection)
Day 1, 5:15 PM: ðŸš¨ ALERT 1: "Unusual S3 access from finance user"
Day 1, 5:45 PM: Attacker downloads database to S3 (Exfiltration)
Day 2, 9:00 AM: Analyst notices 500 GB S3 upload overnight
Day 2, 9:30 AM: Management discovers data breach

Investigation:
â”œâ”€ DETECT (1 hour)
â”‚  â”œâ”€ Alert #1 (Day 1 5:15 PM): Unusual S3 access
â”‚  â”‚  â””â”€ Finance user never accesses S3
â”‚  â”‚  â””â”€ Alert NOT prioritized (lost in noise)
â”‚  â”œâ”€ Alert #2 (Day 2 9:00 AM): 500 GB S3 upload
â”‚  â”‚  â””â”€ Detected in cost anomaly
â”‚  â”‚  â””â”€ Someone finally reviews
â”‚  â””â”€ Ask: What happened during the night?
â”‚     â””â”€ CloudTrail? GuardDuty? VPC Flow Logs?
â”‚
â”œâ”€ INVESTIGATE (1.5 hours)
â”‚  â”œâ”€ CloudTrail timeline:
â”‚  â”‚  â”œâ”€ Day 1 2:15 PM: Developer login (from office)
â”‚  â”‚  â”œâ”€ Day 1 2:30 PM: Developer creates AWS key (API credentials)
â”‚  â”‚  â”œâ”€ Day 1 3:00 PM: Login from offshore IP (attacker)
â”‚  â”‚  â”œâ”€ Day 1 3:15 PM: Finance server access (not developer's usual)
â”‚  â”‚  â”œâ”€ Day 1 3:30 PM: Assume finance IAM role
â”‚  â”‚  â”œâ”€ Day 1 4:00 PM: Finance console login as attacker
â”‚  â”‚  â”œâ”€ Day 1 5:15 PM: RDS database query (SELECT * FROM CUSTOMERS)
â”‚  â”‚  â”œâ”€ Day 1 5:45 PM: S3 upload started (1 million records)
â”‚  â”‚  â””â”€ Day 2 2:00 AM: S3 download (exfiltration to attacker)
â”‚  â”‚
â”‚  â”œâ”€ VPC Flow Logs:
â”‚  â”‚  â”œâ”€ Developer IP (office) â†’ App server (normal)
â”‚  â”‚  â”œâ”€ Offshore IP â†’ App server (SUSPICIOUS)
â”‚  â”‚  â”œâ”€ App server â†’ Finance server (LATERAL MOVEMENT)
â”‚  â”‚  â”œâ”€ Finance server â†’ RDS (database access)
â”‚  â”‚  â””â”€ Finance server â†’ S3 (large upload)
â”‚  â”‚
â”‚  â”œâ”€ Determine scope:
â”‚  â”‚  â”œâ”€ How many records compromised? 1,000,000 customer records
â”‚  â”‚  â”œâ”€ What data? Names, addresses, SSNs, card tokens
â”‚  â”‚  â”œâ”€ Who's affected? ALL customers
â”‚  â”‚  â”œâ”€ When first exposed? Day 1 5:45 PM (17 hours ago)
â”‚  â”‚  â””â”€ How much stolen? 500 GB (full database)
â”‚  â”‚
â”‚  â””â”€ GuardDuty findings:
â”‚     â”œâ”€ T1110 (Brute force) - Attacker tried 1000 password guesses
â”‚     â”œâ”€ T1087 (Account enumeration) - Attacker listed all IAM users
â”‚     â”œâ”€ T1566 (Phishing) - Email from attacker detected
â”‚     â”œâ”€ T1552 (Credentials exposed) - API key found in GitHub
â”‚     â””â”€ T1567 (Data exfiltration) - S3 upload to attacker IP
â”‚
â”œâ”€ CONTAIN (1 hour)
â”‚  â”œâ”€ Revoke ALL developer credentials
â”‚  â”‚  $ aws iam delete-access-key --user-name developer --access-key-id AKIA...
â”‚  â”œâ”€ Disable developer IAM user account
â”‚  â”‚  $ aws iam update-user --user-name developer --access-key-ids-disabled
â”‚  â”œâ”€ Force password reset
â”‚  â”‚  $ aws iam create-login-profile --user-name developer --password (temporary) --password-reset-required
â”‚  â”œâ”€ Isolate finance server
â”‚  â”‚  â”œâ”€ Remove from network (Security Group: no inbound/outbound)
â”‚  â”‚  â”œâ”€ Stop RDS database access (modify security group)
â”‚  â”‚  â””â”€ Disable finance IAM role
â”‚  â”œâ”€ Block attacker IP globally
â”‚  â”‚  â”œâ”€ WAF: IP in blocklist
â”‚  â”‚  â”œâ”€ NACLs: Deny ingress from IP
â”‚  â”‚  â””â”€ VPC Flow Logs: Monitor if they try again
â”‚  â””â”€ Delete S3 bucket with exfiltrated data
â”‚     $ aws s3 rm s3://bucket --recursive
â”‚
â”œâ”€ ERADICATE (1.5 hours)
â”‚  â”œâ”€ Re-image developer laptop
â”‚  â”‚  â””â”€ Remove malware/backdoor
â”‚  â”œâ”€ Force password changes
â”‚  â”‚  â”œâ”€ Developer (new password)
â”‚  â”‚  â”œâ”€ Finance team (new passwords)
â”‚  â”‚  â”œâ”€ Database admin (new password)
â”‚  â”‚  â””â”€ All SSH keys, API keys, tokens
â”‚  â”œâ”€ Patch vulnerabilities
â”‚  â”‚  â””â”€ The phishing link exploited a 0-day
â”‚  â”‚  â””â”€ Deploy patch or workaround
â”‚  â”œâ”€ Re-enable but monitor
â”‚  â”‚  â””â”€ Finance server: Re-enable with monitoring
â”‚  â”‚  â””â”€ Alert on unusual commands
â”‚  â”‚  â””â”€ Check for persistence mechanism
â”‚  â””â”€ Kill persistent backdoor
â”‚     â””â”€ If malware remains, remove it completely
â”‚
â”œâ”€ RECOVER (1 hour)
â”‚  â”œâ”€ Restore from backups (pre-compromise)
â”‚  â”‚  â””â”€ Day 0 11:00 AM backup (before attack)
â”‚  â”‚  â””â”€ Restore database from backup
â”‚  â”œâ”€ Verify integrity
â”‚  â”‚  â””â”€ Data matches expected state
â”‚  â”‚  â””â”€ No signs of tampering
â”‚  â””â”€ Test applications
â”‚     â””â”€ Everything working normally?
â”‚
â””â”€ COMMUNICATE (2+ hours)
   â”œâ”€ Timeline notification:
   â”‚  â”œâ”€ Hour 0: "We're investigating anomalies"
   â”‚  â”œâ”€ Hour 1: "We've confirmed a compromise"
   â”‚  â”œâ”€ Hour 2: "Attacker had access for 17 hours"
   â”‚  â”œâ”€ Hour 3: "1 million customers affected"
   â”‚  â””â”€ Hour 4: "Service restored, investigation ongoing"
   â”œâ”€ Customer notification:
   â”‚  â””â”€ Email: "We experienced unauthorized access to your data"
   â”‚  â””â”€ "Data exposed: name, SSN, card tokens"
   â”‚  â””â”€ "Timeline: [date] [hour]"
   â”‚  â””â”€ "We have credit monitoring service"
   â”‚  â””â”€ "Hotline for questions: 1-800-BREACH"
   â”œâ”€ Regulatory notification:
   â”‚  â””â”€ GDPR: 72 hours
   â”‚  â””â”€ State AG: Within 45-60 days
   â”‚  â””â”€ Media: Will leak eventually
   â”œâ”€ Press release:
   â”‚  â””â”€ "We experienced a cybersecurity incident"
   â”‚  â””â”€ "We've restored service and enhanced security"
   â”‚  â””â”€ "Our investigation is ongoing"
   â””â”€ Financial impact:
      â”œâ”€ Revenue: $50M+ (customers leave)
      â”œâ”€ Fines: $10M+ (GDPR, state AG, HIPAA)
      â”œâ”€ Credit monitoring: $2M
      â”œâ”€ Remediation: $5M (new security tools, personnel)
      â”œâ”€ Legal: $3M
      â”œâ”€ Reputation: Priceless (takes 2+ years to recover)
      â””â”€ TOTAL: $70M+ impact

Resume Bullets:
â€¢ "Investigated and contained multi-stage breach affecting 1M customers in 3 hours"
â€¢ "Performed root cause analysis identifying vulnerability in phishing email security"
â€¢ "Automated detection of lateral movement with VPC Flow Logs, preventing 80% of similar attacks"
â€¢ "Implemented zero-trust network architecture, segmenting critical systems post-incident"
```

---

### Scenario 10: Incident Response Failure (What Went Wrong?)

**Difficulty:** Hard | **Time:** 2 hours | **Impact:** CRITICAL (Organizational)

**The Scenario:**
```
Your incident response failed. What went wrong?

Real-World Examples:
â”œâ”€ Target Breach (2013)
â”‚  â”œâ”€ HVAC contractor compromised
â”‚  â”œâ”€ Attacker accessed Target network
â”‚  â”œâ”€ Installed malware on payment systems
â”‚  â”œâ”€ Stole 40 million card numbers
â”‚  â”œâ”€ Detection: 2+ months later (by card companies)
â”‚  â”œâ”€ Response: Failed (no one noticed until outside alert)
â”‚  â”œâ”€ Cost: $18.5 million settlement
â”‚  â””â”€ Lessons:
â”‚     â”œâ”€ Supply chain security (didn't vet contractor's security)
â”‚     â”œâ”€ Network segmentation (payment systems on same network)
â”‚     â”œâ”€ Threat hunting (didn't look for malware)
â”‚     â””â”€ Incident response (no process to handle it)
â”‚
â”œâ”€ Equifax Breach (2017)
â”‚  â”œâ”€ Vulnerability (Apache Struts) known for weeks
â”‚  â”œâ”€ Patch available but not applied
â”‚  â”œâ”€ Attacker exploited vulnerability
â”‚  â”œâ”€ Accessed 145 million records (social security numbers!)
â”‚  â”œâ”€ Detection: 2+ months later (forensic investigation)
â”‚  â”œâ”€ Response: Terrible (executives sold stock before disclosure)
â”‚  â”œâ”€ Cost: $700 million settlement + reputation destroyed
â”‚  â””â”€ Lessons:
â”‚     â”œâ”€ Patch management (didn't apply available patch)
â”‚     â”œâ”€ Detection (monitoring too late)
â”‚     â”œâ”€ Response (delayed disclosure, insider trading)
â”‚     â””â”€ Executive accountability (criminal charges)
â”‚
â”œâ”€ Uber Breach (2016)
â”‚  â”œâ”€ Attacker accessed GitHub private repo
â”‚  â”œâ”€ Found AWS credentials
â”‚  â”œâ”€ Accessed 57 million user records
â”‚  â”œâ”€ Response: Covered up breach for 1 year
â”‚  â”œâ”€ Cost: $100M settlement + reputation + management changes
â”‚  â””â”€ Lessons:
â”‚     â”œâ”€ Secrets management (credentials in GitHub = bad)
â”‚     â”œâ”€ Breach disclosure (hiding = illegal)
â”‚     â””â”€ Corporate accountability (CEO forced to resign)
â”‚
â””â”€ Twitch Breach (2021)
   â”œâ”€ Data stolen by insider
   â”œâ”€ 125 GB of data exposed
   â”œâ”€ Source code, creator earnings, payment info
   â”œâ”€ Detection: Through leaked data disclosure
   â”œâ”€ Response: Took 2 weeks to acknowledge
   â”œâ”€ Cost: Reputation damage, creator trust lost
   â””â”€ Lessons:
      â”œâ”€ Insider threat program (didn't detect)
      â”œâ”€ Data loss prevention (large transfers not flagged)
      â””â”€ Transparency (delayed response lost trust)

Common Incident Response Failures:

âŒ FAILURE #1: No incident response plan
   â”œâ”€ Problem: When breach happens, people panic
   â”œâ”€ Everyone asking: "What do I do?"
   â”œâ”€ Slow response, missed evidence
   â””â”€ Fix: Create playbooks BEFORE incident

âŒ FAILURE #2: No detection capability
   â”œâ”€ Problem: Attacker in system for months, nobody notices
   â”œâ”€ No SIEM, no monitoring, no alerts
   â”œâ”€ Find out from external researcher or media
   â””â”€ Fix: Deploy SIEM, GuardDuty, CloudTrail NOW

âŒ FAILURE #3: No detection threshold
   â”œâ”€ Problem: Millions of alerts, analysts ignore them
   â”œâ”€ Tuning too sensitive = noise, tuning too loose = misses attacks
   â”œâ”€ "Alert fatigue" = ignored alerts
   â””â”€ Fix: Tune alerts carefully, start with high-confidence rules

âŒ FAILURE #4: Slow containment
   â”œâ”€ Problem: Identify breach on day 2, attacker escapes
   â”œâ”€ Didn't revoke credentials fast enough
   â”œâ”€ Didn't isolate network in time
   â””â”€ Fix: Pre-written playbooks, practice regularly

âŒ FAILURE #5: No evidence collection
   â”œâ”€ Problem: "We need to analyze what happened"
   â”œâ”€ Forgot to preserve logs, capture memory
   â”œâ”€ By time forensics team arrives, evidence gone
   â””â”€ Fix: Automate evidence collection (CloudTrail, VPC Flow Logs)

âŒ FAILURE #6: Poor communication
   â”œâ”€ Problem: Executives don't know what's happening
   â”œâ”€ Sales team keeps selling, marketing doesn't know to pause
   â”œâ”€ Customer service gets flooded with "Is my data exposed?" calls
   â””â”€ Fix: Pre-defined communication templates for each scenario

âŒ FAILURE #7: Slow regulatory notification
   â”œâ”€ Problem: Late GDPR/HIPAA disclosure = $10M+ fines
   â”œâ”€ Didn't know 72-hour deadline (GDPR)
   â”œâ”€ Didn't know 60-day deadline (HIPAA)
   â””â”€ Fix: Put deadlines in calendar, involve legal from hour 1

âŒ FAILURE #8: No lessons learned
   â”œâ”€ Problem: Same attack hits company 6 months later
   â”œâ”€ "We forgot what we learned"
   â”œâ”€ No post-incident review (PIR)
   â””â”€ Fix: Mandatory post-incident review within 1 week

Learning Opportunities:

âœ… What went right (in Scenario 10 failure):
   â”œâ”€ We had a backup (could restore)
   â”œâ”€ We had logs (could investigate)
   â”œâ”€ We had insurance (could pay for response)
   â””â”€ We survived (business still operating)

âœ… Lessons for your organization:
   â”œâ”€ Create incident response playbook (TODAY)
   â”œâ”€ Deploy SIEM/GuardDuty (THIS WEEK)
   â”œâ”€ Practice incident drills (MONTHLY)
   â”œâ”€ Tune alert thresholds (QUARTERLY)
   â”œâ”€ Automate evidence collection (NOW)
   â”œâ”€ Create communication templates (THIS MONTH)
   â”œâ”€ Involve legal team (IN EVERY RESPONSE)
   â””â”€ Document lessons learned (AFTER EVERY INCIDENT)

Resume Bullet from Learning Incident Response Failures:
â€¢ "Analyzed post-incident reviews from 3 high-profile breaches, identified and remediated same vulnerabilities in our environment"
â€¢ "Implemented incident response playbook reducing MTTR by 70%, prepared organization for worst-case scenarios"
â€¢ "Established incident response drills and post-incident reviews, preventing recurrence of similar attacks"
```

---

## ðŸ“Š Complete Incident Scenario Comparison Matrix

| Scenario | Difficulty | MTTD | MTTR | Prevention | Resume Impact | Time |
|----------|-----------|------|------|-----------|---------------|------|
| 1. Brute Force | Easy | <5 min | 15 min | MFA, Account lockout | Medium | 30 min |
| 2. Ransomware | Medium | <30 min | 45 min | Backups, Isolation | High | 1 hr |
| 3. Insider Threat | Hard | <30 min | 2+ hrs | DLP, Access controls | Very High | 2 hrs |
| 4. Supply Chain | Very Hard | <1 hr | 2+ hrs | Dependency scanning | Very High | 2-3 hrs |
| 5. API Attack | Medium | <10 min | 20 min | API rate limiting, Secrets mgmt | High | 1 hr |
| 6. DDoS | Medium | 1 min | 30 min | WAF, Auto scaling | High | 1 hr |
| 7. Misconfiguration | Easy | <5 min | 10 min | Config rules, SCPs | Critical | 30 min |
| 8. Compliance | Medium | <1 hr | 3-4 hrs | Continuous compliance | High | 2 hrs |
| 9. Multi-Stage | Very Hard | 1+ hrs | 3+ hrs | All controls working together | Critical | 3+ hrs |
| 10. Failure Analysis | Hard | N/A (lesson) | N/A | All of above | High | 2 hrs |

---

## ðŸŽ“ How to Use These Scenarios

### Method 1: Tabletop Exercise (Classroom/Team)
```
1. Assign roles:
   - Incident Commander (makes decisions)
   - SOC Analyst (detects/investigates)
   - Response Team Lead (executes playbook)
   - Documentation Lead (records timeline)
   - Executive Sponsor (communication)

2. Read scenario aloud
3. Pause at decision points: "What do you do next?"
4. Discuss options
5. Walk through response
6. Debrief: What did we do well? What could improve?
```

### Method 2: Solo Practice (Self-Study)
```
1. Read scenario
2. Write down your response plan
3. Check against suggested response
4. Note gaps
5. Update your incident response playbook
```

### Method 3: Hands-On Lab (Actual Simulation)
```
1. Set up sandbox AWS account
2. Actually recreate the scenario:
   - Use Stratus Red Team for attacks
   - Deploy test apps and databases
   - Create realistic data
3. Practice detecting and responding
4. Time yourself (MTTD/MTTR)
5. Compare to best practices
6. Document metrics
```

---

**All 10 Scenarios Complete! ðŸŽ‰**

**Next Steps:**
1. Read through all 10 scenarios this week
2. Pick 1-2 for team tabletop exercise
3. Deploy sandbox (Module 9)
4. Run hands-on labs for 3 scenarios
5. Time your team's MTTD/MTTR
6. Update your incident response playbook

**Move to Module 6 for Purple Team & MITRE ATT&CK Framework ðŸš€**
